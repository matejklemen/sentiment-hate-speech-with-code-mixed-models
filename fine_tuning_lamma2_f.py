# -*- coding: utf-8 -*-
"""Fine_Tuning_Lamma2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ihSLebNokOd9ZLai8ZiQimDuzp07djGY

## Setup

We will need  libraries such as `hugging_face`, `transformers`,`accelerate`, `peft`, `datasets` and `TRL` to use the `SFTTrainer`. We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes).We will also install einops as it is a requirement to load the models.
"""

# from google.colab import drive
# drive.mount('/content/drive')

# !pip install datasets
# !pip install -q huggingface_hub
# !pip install -q -U trl transformers accelerate peft
# !pip install -q -U datasets bitsandbytes einops wandb
# Uncomment to install new features that support latest models like Llama 2
# !pip install git+https://github.com/huggingface/peft.git
# !pip install git+https://github.com/huggingface/transformers.git

"""## Dataset"""

# # When prompted, paste the HF access token you created earlier.
from huggingface_hub import notebook_login
notebook_login()

from datasets import load_dataset
# dataset_name = "Tngarg/tamil_english"
# dataset_name = "Tngarg/russian_english"
dataset_name = "Tngarg/french_english"
# dataset_name = "Tngarg/Hindi_english"

dataset = load_dataset(dataset_name)

dataset['train'][1]

from datasets import Dataset
dataset = dataset.map(lambda example:{'prompt' : str('Input : '+ example['tweet']+ ' \n\n### sentiment : '+ example['sentiment'])})

dataset

dataset['train'][1]

"""## Loading the Model and tokenizer"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, AutoModel

model_name = "meta-llama/Llama-2-7b-hf"

# function to load model and tokenizer
def load_model(model_name, bnb_config):
    n_gpus = torch.cuda.device_count()
    max_memory = f'{40960}MB'

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
    #     device_map = {"": 0}, # dispatch efficiently the model on the available ressources
    #     max_memory = {i: max_memory for i in range(n_gpus)},
    )

    # model.config.use_cache = False

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
    tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer

def create_bnb_config():
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )
    return bnb_config

bnb_config = create_bnb_config()
model, tokenizer = load_model(model_name, bnb_config)

"""## Training : Configuring PEFT, Lora, Trainer"""

from peft import LoraConfig

def create_peft_config(modules):
    """
    Create Parameter-Efficient Fine-Tuning config for your model
    """
    config = LoraConfig(
        r=16,  # dimension of the updated matrices
        lora_alpha=64,  # parameter for scaling
        target_modules=modules,
        lora_dropout=0.1,  # dropout probability for layers
        bias="none",
        task_type="CAUSAL_LM",
    )

    return config

# peft_config = create_peft_config()

from transformers import TrainingArguments

output_dir = "outputs"
per_device_train_batch_size = 1
gradient_accumulation_steps = 4
optim = "paged_adamw_8bit"
save_steps = 1
num_train_epochs = 4
logging_steps = 1
learning_rate = 2e-4
max_grad_norm = 0.3
max_steps = 20
warmup_ratio = 0.03
warmup_steps=2,
lr_scheduler_type = "linear"

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    num_train_epochs=num_train_epochs,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    fp16=True,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    warmup_steps=2,
    group_by_length=True,
    lr_scheduler_type=lr_scheduler_type,
    # use_reentrant=True
)

import bitsandbytes as bnb
def find_all_linear_names(model):
    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])

    if 'lm_head' in lora_module_names:  # needed for 16-bit
        lora_module_names.remove('lm_head')
    return list(lora_module_names)

from peft import LoraConfig, get_peft_model
from peft import prepare_model_for_kbit_training
model.gradient_checkpointing_enable()
# 2 - Using the prepare_model_for_kbit_training method from PEFT
model = prepare_model_for_kbit_training(model)
# Get lora module names
modules = find_all_linear_names(model)
print(modules)
# Create PEFT config for these modules and wrap the model to PEFT
peft_config = create_peft_config(modules)

model = get_peft_model(model, peft_config)

from trl import SFTTrainer

max_seq_length = 2048

dtypes = {}
for _, p in model.named_parameters():
    dtype = p.dtype
    if dtype not in dtypes: dtypes[dtype] = 0
    dtypes[dtype] += p.numel()
total = 0
for k, v in dtypes.items(): total+= v
for k, v in dtypes.items():
    print(k, v, v/total)

do_train = True

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset['train'],
    peft_config=peft_config,
    dataset_text_field="prompt",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
)

# for name, module in trainer.model.named_modules():
#     if "norm" in name:
#         module = module.to(torch.float32)

do_train = True

# Launch training
print("Training...")

if do_train:
    train_result = trainer.train()
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()
    print(metrics)

"""## Testing"""

dataset['train']['prompt'][6]

device = "cuda:0"
text = dataset['train']['tweet'][6]
inputs = tokenizer(text, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

def output_function(sample):
    inputs = tokenizer(sample,return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=50 )
    # outputs = model.generate(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"], max_new_tokens=50 )
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result

import pandas as pd
import swifter
df = pd.DataFrame(dataset['test'])
df = df.head(300)
df['output'] = df['tweet'].swifter.apply(output_function)
# df.to_csv('conda/envs/gpu_env/Cross_lingual/final_codes/laama2_tamil_eng.csv')
# df.to_csv('conda/envs/gpu_env/Cross_lingual/final_codes/laama2_russian_eng.csv')
df.to_csv('conda/envs/gpu_env/Cross_lingual/final_codes/laama2_french_eng.csv')
# df.to_csv('conda/envs/gpu_env/Cross_lingual/final_codes/laama2_Hindi_eng.csv')

"""## Saving the Model"""

# import os
# output_dir = "/content/drive/MyDrive/Slovenia/Final Datasets/Lamma_results/tamil"
# # Saving model
# print("Saving last checkpoint of the model...")
# os.makedirs(output_dir, exist_ok=True)
# trainer.model.save_pretrained(output_dir)

# from huggingface_hub import login
# login()

# model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training
# model_to_save.save_pretrained("outputs")
# lora_config = LoraConfig.from_pretrained('outputs')
# model.push_to_hub("Tngarg/lamma2_tamil_english",create_pr=1)

# model_dir = "/content/drive/MyDrive/Slovenia/Final Datasets/Lamma_results/tamil"
# from peft import PeftModel, PeftConfig
# from transformers import AutoModel

# config = PeftConfig.from_pretrained("/content/drive/MyDrive/Slovenia/Final Datasets/Lamma_results/tamil")
# model = AutoModel.from_pretrained("/content/drive/MyDrive/Slovenia/Final Datasets/Lamma_results/tamil")
# model = PeftModel.from_pretrained(model, "/content/drive/MyDrive/Slovenia/Final Datasets/Lamma_results/tamil")

